# crawl_task.py
# ğŸŒ ìƒí’ˆ í˜ì´ì§€ë¥¼ í¬ë¡¤ë§í•˜ì—¬ í•„ìš”í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³ , ì¶”ë¡  íƒœìŠ¤í¬ë¡œ ë„˜ê¸°ëŠ” ì—­í• ì„ í•¨

from task_queue_module.task_queue import celery_app       # Celery ì¸ìŠ¤í„´ìŠ¤
from crawler.dispatcher import route_to_parser            # URL ë¼ìš°íŒ… â†’ í¬ë¡¤ë§ íŒŒì„œ ê²°ì •
from crawler.fetch_html import create_driver              # ì…€ë ˆë‹ˆì›€ í¬ë¡¬ ë“œë¼ì´ë²„ ìƒì„±ê¸°
from task_queue_module.infer_task import process_inference # ì¶”ë¡  íë¡œ ê²°ê³¼ ë„˜ê¹€

# âœ… "crawl_queue"ì—ì„œ ì‹¤í–‰ë  íƒœìŠ¤í¬ ì •ì˜
# - URLì— ë”°ë¼ Kakao Gift, Kakao Store, Naver Smartstore ë“±ì—ì„œ í¬ë¡¤ë§ ìˆ˜í–‰
@celery_app.task(queue="crawl_queue", bind=True, autoretry_for=(Exception,), retry_kwargs={"max_retries": 3}, retry_backoff=True)
def crawl_product(self, doc_id: str, product: dict):
    url = product.get("link")
    driver = create_driver()

    try:
        result = route_to_parser(url, driver)  # âœ… ì´ í•¨ìˆ˜ê°€ Noneì„ ë°˜í™˜í•  ìˆ˜ë„ ìˆìŒ

        # ğŸš¨ fallback ì²˜ë¦¬ ì—†ì´ inference í˜¸ì¶œ ê°€ëŠ¥ (inference ë‚´ë¶€ì—ì„œ ì²˜ë¦¬í•˜ë‹ˆê¹Œ)
        process_inference.delay(doc_id, product, result)

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ íƒœìŠ¤í¬ ì‹¤íŒ¨ (doc_id: {doc_id}, url: {url}) â†’ {e}")
        raise e  # autoretry_forì„ ìœ„í•´ ë°˜ë“œì‹œ raise

    finally:
        driver.quit()
