import json
import pandas as pd
import concurrent.futures  # ë³‘ë ¬ ì‹¤í–‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

# âœ… 1ï¸âƒ£ í¬ë¡¬ ì˜µì…˜ ì„¤ì •
chrome_options = Options()
chrome_options.add_argument("--headless=new")  # ìµœì‹  Headless ëª¨ë“œ (ì†ë„ í–¥ìƒ)
chrome_options.add_argument("--no-sandbox")
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--disable-gpu")
chrome_options.add_argument("--log-level=3")
chrome_options.add_argument("--window-size=1920x1080")
chrome_options.add_argument("--disable-blink-features=AutomationControlled")
chrome_options.add_argument("--blink-settings=imagesEnabled=false")
chrome_options.add_argument("--disable-extensions")
chrome_options.add_argument("--disable-infobars")

# âœ… 2ï¸âƒ£ JSON íŒŒì¼ì—ì„œ URL ê°€ì ¸ì˜¤ê¸°
json_file_path = "kakao_url.json"  # JSON íŒŒì¼ ê²½ë¡œ
with open(json_file_path, "r", encoding="utf-8") as file:
    data = json.load(file)

# âœ… 3ï¸âƒ£ store.kakao.comì—ì„œë§Œ URL ì¶”ì¶œ
urls = [product["link"] for entry in data if "products" in entry for product in entry["products"] if "link" in product and product["link"].startswith("https://store.kakao.com")]

# âœ… 4ï¸âƒ£ í¬ë¡¤ë§ ì‹¤í–‰ í•¨ìˆ˜ (ë³‘ë ¬ ì‹¤í–‰)
def crawl_category(url):
    """ê° URLì„ í¬ë¡¤ë§í•˜ì—¬ ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=chrome_options)

    driver.get(url)

    # ğŸš€ WebDriverWaitìœ¼ë¡œ ìš”ì†Œê°€ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼
    try:
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "#mArticle > div > div.product_section > app-view-product-category-path > div > div > a"))
        )
    except:
        print(f"âŒ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
        driver.quit()
        return {"URL": url, "ì¹´í…Œê³ ë¦¬ ì „ì²´ ê²½ë¡œ": "ì—†ìŒ"}

    # âœ… BeautifulSoupìœ¼ë¡œ HTML íŒŒì‹±
    html = driver.page_source
    soup = BeautifulSoup(html, "html.parser")

    # âœ… ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    category_elements = soup.select("#mArticle > div > div.product_section > app-view-product-category-path > div > div > a")

    # âœ… í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥
    categories = [category.text.strip() for category in category_elements]

    # âœ… ì¹´í…Œê³ ë¦¬ ì „ì²´ ê²½ë¡œ (ì²« ë²ˆì§¸ ì—´)
    category_full_path = " > ".join(categories)

    # âœ… ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì €ì¥ (ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ìœ ë™ì  ëŒ€ì‘)
    category_dict = {"URL": url, "ì¹´í…Œê³ ë¦¬ ì „ì²´ ê²½ë¡œ": category_full_path}
    print(category_dict)
    # âœ… ê°œë³„ ì¹´í…Œê³ ë¦¬ ì—´ ì¶”ê°€ (ìœ ë™ì )
    for i, category in enumerate(categories):
        category_dict[f"ì¹´í…Œê³ ë¦¬_{i+1}"] = category

    driver.quit()
    return category_dict

# âœ… 5ï¸âƒ£ ë³‘ë ¬ ì²˜ë¦¬ (ìµœëŒ€ 5ê°œì”© ë™ì‹œ ì‹¤í–‰)
results = []
with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
    futures = {executor.submit(crawl_category, url): url for url in urls}
    for future in concurrent.futures.as_completed(futures):
        results.append(future.result())

# âœ… 6ï¸âƒ£ DataFrameìœ¼ë¡œ ë³€í™˜ (ìƒí’ˆë³„ ì¹´í…Œê³ ë¦¬ ê°œìˆ˜ ë‹¤ë¦„ì„ ê³ ë ¤)
df = pd.DataFrame(results)

# âœ… 7ï¸âƒ£ CSV íŒŒì¼ ì €ì¥
df.to_csv("ì¹´í…Œê³ ë¦¬_ê²½ë¡œ.csv", index=False, encoding="utf-8-sig")

# âœ… ğŸ”Ÿ ì¶œë ¥ í™•ì¸
print("âœ… í¬ë¡¤ë§ ì™„ë£Œ! ì €ì¥ëœ ë°ì´í„°:")
print(df.head())
